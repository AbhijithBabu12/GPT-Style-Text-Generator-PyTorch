{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouDFBk0zh_5n",
        "outputId": "d18627dd-1687-4167-9e3e-2f60e5e66ff8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 2.8.0+cu126\n",
            "Uninstalling torch-2.8.0+cu126:\n",
            "  Successfully uninstalled torch-2.8.0+cu126\n",
            "\u001b[33mWARNING: Skipping torchtext as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: torchvision 0.23.0+cu126\n",
            "Uninstalling torchvision-0.23.0+cu126:\n",
            "  Successfully uninstalled torchvision-0.23.0+cu126\n",
            "Found existing installation: torchaudio 2.8.0+cu126\n",
            "Uninstalling torchaudio-2.8.0+cu126:\n",
            "  Successfully uninstalled torchaudio-2.8.0+cu126\n",
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m126.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m137.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m112.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "5# -------------------------------\n",
        "# 1️⃣ Uninstall conflicting versions\n",
        "# -------------------------------\n",
        "!pip uninstall -y torch torchtext torchvision torchaudio numpy\n",
        "\n",
        "# -------------------------------\n",
        "# 2️⃣ Install compatible versions\n",
        "# For vanilla RNN + IMDB\n",
        "# torch 2.3.0, torchtext 0.18.0, torchvision/torchaudio matching\n",
        "# numpy 1.26.4 (avoids PyTorch errors)\n",
        "# -------------------------------\n",
        "!pip install torch==2.3.0 torchtext==0.18.0 torchvision==0.18.0 torchaudio==2.3.0 numpy==1.26.4 --quiet\n",
        "\n",
        "# -------------------------------\n",
        "# 3️⃣ Restart runtime (required to load new versions)\n",
        "# -------------------------------\n",
        "import os\n",
        "os.kill(os.getpid(), 9)  # This forces Colab to restart\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NCCSr37laGG",
        "outputId": "fbac10e3-1fba-4300-96bd-c494d2f3aec8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cXedoF32lctF"
      },
      "outputs": [],
      "source": [
        "!cp \"/content/drive/MyDrive/Tensorflow/movie.csv\" /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i8UoaJOCihLB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('movie.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "E2Z8FOpRjpOU",
        "outputId": "0eab2ccc-7b03-4eff-a6bc-e24a35fd4ced"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 34886,\n  \"fields\": [\n    {\n      \"column\": \"Release Year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 27,\n        \"min\": 1901,\n        \"max\": 2017,\n        \"num_unique_values\": 117,\n        \"samples\": [\n          1945,\n          1905,\n          1954\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 32432,\n        \"samples\": [\n          \"Mayalodu\",\n          \"Dikari\",\n          \"En Rathathin Rathame\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Origin/Ethnicity\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 24,\n        \"samples\": [\n          \"Filipino\",\n          \"Tamil\",\n          \"American\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Director\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 12593,\n        \"samples\": [\n          \"B. S. Narayana\",\n          \"Ravi Kinnagi\",\n          \"Steve Buscemi\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Cast\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 32182,\n        \"samples\": [\n          \"Sivaji Ganesan, K. R. Vijaya, Sripriya\",\n          \"Corbin Bernsen, Dennis Haysbert, Scott Bakula, Bob Uecker\",\n          \"Paul Douglas, Richard Basehart\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Genre\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2265,\n        \"samples\": [\n          \"comedy/drama/action\",\n          \"drama / suspense / fantasy / costume\",\n          \"comedy, anthology\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Wiki Page\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 34070,\n        \"samples\": [\n          \"https://en.wikipedia.org/wiki/Two_Thousand_and_None\",\n          \"https://en.wikipedia.org/wiki/Death_Note_2:_The_Last_Name\",\n          \"https://en.wikipedia.org/wiki/Contagion_(film)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Plot\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 33869,\n        \"samples\": [\n          \"In Spain 1519, two con artists, Tulio and Miguel, win a map to the legendary City of Gold, El Dorado, in a rigged dice gamble. After their con is exposed, the two evade the guards and hide aboard one of the ships to be led by Hern\\u00e1n Cort\\u00e9s for the New World. During the voyage, they are caught as stowaways and imprisoned, but break free and take a rowboat with the help of Cort\\u00e9s' horse, Altivo.\\r\\nThey reach an unknown shore at the edge of Mexico, where Miguel begins to recognize landmarks from the map, leading them to a totem marker near a waterfall that Tulio believes is a dead end. As they prepare to leave, they encounter a native woman, Chel, being chased by guards. When the guards see Tulio and Miguel riding Altivo as depicted on the totem, they escort them and Chel to a secret entrance behind the falls, into El Dorado. They are brought to the city's elders, kindhearted Chief Tannabok and wicked high priest Tzekel-Kan. The pair are mistaken for gods and given luxurious quarters, along with the charge of Chel. Chel discovers the two are conning the people but promises to remain quiet if they take her with them when they leave the city. The two are showered with gifts of gold from Tannabok, but disapprove of Tzekel-Kan attempting to sacrifice a civilian as the gods' ritual.\\r\\nTulio instructs Tannabok to build them a boat so that they can leave the city with all the gifts they have been given. During the three days this will take, Miguel explores the city, and Chel gets romantically close to Tulio. Miguel comes to appreciate the peaceful life the citizens seem to enjoy. When Tzekel-Kan sees Miguel playing a ball game with children, he insists the gods demonstrate their powers against the city's best players in the same game. Tulio and Miguel are far outmatched, but Chel is able to substitute the ball with an armadillo, allowing them to win. Miguel spares the ritual of sacrificing the losing team and chastises Tzekel-Kan, much to the crowd's approval. Tzekel-Kan notices Miguel received a small cut and realizes the two are not gods, because gods do not bleed. Tzekel-Kan conjures a giant stone jaguar to chase them through the city. Tulio and Miguel outwit the jaguar, causing it and Tzekel-Kan to fall into a giant whirlpool, thought to be the entrance to Xibalba. Tzekel-Kan then surfaces in the jungle, where he encounters Cort\\u00e9s and his men. Thinking Cort\\u00e9s is a god, he offers to lead them to El Dorado.\\r\\nWith the boat completed, Miguel says he will stay in the city. As Tulio and Chel board the boat, they see smoke on the horizon and realize Cort\\u00e9s is close. Tulio suggests using the boat to ram rock pillars under the waterfall and block the main entrance to the city. The plan succeeds with the citizens pulling over a statue in the boat's wake to give it enough speed. As the statue starts to fall too quickly, Tulio has difficulty in preparing the boat's sail. Giving up on staying in the city, Miguel and Altivo jump onto the boat to unfurl the sails, assuring the boat clears the statue in time. The group successfully crashes against the pillars, causing a cave-in but losing all their gifts in the process. They hide near the totem, just as Cort\\u00e9s' men and Tzekel-Kan arrive. When Tzekel-Kan finds the entrance blocked, Cort\\u00e9s brands him a liar, and takes Tzekel-Kan prisoner as they leave.\\r\\nTulio and Miguel, though disappointed they lost the gold (unaware that Altivo still wears the golden horseshoes with which he was outfitted in El Dorado), head in a different direction for a new adventure with Chel.\",\n          \"First Lieutenant Shunichi Maki of the Japan Air Self-Defense Force is a prestigious F-15 Eagle jet pilot. A lifelong fan of flying since he was a child, being a pilot is his ultimate dream. Unfortunately, his duties distance himself from his wife, Yoko, who feels neglected, and his son, Tsugumu, who has a possibly terminal congenital blood disease.\\r\\nMaki decides to quit the Air Force to devote more time to his family and to spend whatever time is left with his son. He takes a part-time job as a commercial tour guide for a kindly group of people who allow him time to take care of his family.\\r\\nPrior to quitting, Maki and his flight partner Yamashima are alerted to a strange red light streaking towards Japan, and Maki's plane passes through the red light seemingly without any damage. He suffers no ill effects other than strange images briefly playing out in his mind. He later discovers that the images are telepathic messages from a strange being that exists in the red light.\",\n          \"Sergeant Deadhead is a bumbling soldier who is sent to the guardhouse for blowing up a model rocket on the parade ground of the air base where he is stationed. His fianc\\u00e9e, Airman Lucy Turner despairs of ever marrying him because of him being constantly disciplined for his antics. She is worried that she will have to marry him while he is in the guardhouse.\\r\\nTogether with Private McEvoy, Sergeant Deadhead escapes from the guardhouse. Private McEvoy decides to break back in, but Sergeant Deadhead hides in a nearby space rocket, not knowing it is set to blast off with a chimpanzee aboard. He falls asleep in the rocket's control room and is accidentally blasted into space, together with the chimpanzee.\\r\\nWhen Sergeant Deadhead is discovered to be aboard the rocket, General Fogg decides to spin the facts and say that Sergeant Deadhead volunteered for the mission. He and Navy Captain Weiskopf also decide that Sergeant Deadhead and Airman Turner will have a well publicized wedding on the air base when Sergeant Deadhead returns to earth.\\r\\nWhen Sergeant Deadhead returns home he is a national hero but has also developed a massive ego due to space travel causing his personality to blend with that of the chimpanzee, and the realization that he has become a media sensation.\\r\\nA soldier who looks exactly like him, Sergeant Donovan, is found to take his place. When the smooth talking Sergeant Donovan is set to take Sergeant Deadhead's place at the altar, Sergeant Deadhead breaks out of the guardhouse, starts to recover his personality, and switches places with Sergeant Donovan. When the leadership realizes he has escaped the guardhouse, he runs away.\\r\\nSergeant Deadhead finds out about Sergeant Donovan, and goes to the hotel where the wedding reception and honeymoon are taking place. There he switches places with Sergeant Donovan to enjoy his honeymoon. However, General Fogg and the others find Donovan and take him back to the honeymoon suite, looking for Deadhead. Thinking Donovan is Deadhead, Airman Turner chases them out.\\r\\nWhen Airman Turner is preparing a bath for Donovan, Deadhead sneaks in the window and coldcocks Donovan with a vase, knocking him out. Fogg, Weiskopf and the others show up again at the door, claiming to have a message from the President. Thinking that Deadhead is Donovan, Fogg and Weiskopf have two MP's escort Deadhead and Turner to the airport to fly to see the President. Donovan awakes alone in the closet, then goes back to base where Fogg and Weiskopf discover that Deadhead is with Turner and going to meet the President. Deadhead and Turner enjoy the rest of their honeymoon and their meeting with the President.\\r\\nTwo marines appear to arrest Deadhead but mistakenly arrest the President, who is trying on Deadhead's space helmet. Deadhead and Turner escape in a White House helicopter. Fogg, Weiskopf and the others end up in the guardhouse.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-d62052b1-45bf-4574-b51a-793be2d7f774\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Release Year</th>\n",
              "      <th>Title</th>\n",
              "      <th>Origin/Ethnicity</th>\n",
              "      <th>Director</th>\n",
              "      <th>Cast</th>\n",
              "      <th>Genre</th>\n",
              "      <th>Wiki Page</th>\n",
              "      <th>Plot</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1901</td>\n",
              "      <td>Kansas Saloon Smashers</td>\n",
              "      <td>American</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>NaN</td>\n",
              "      <td>unknown</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Kansas_Saloon_Sm...</td>\n",
              "      <td>A bartender is working at a saloon, serving dr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1901</td>\n",
              "      <td>Love by the Light of the Moon</td>\n",
              "      <td>American</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>NaN</td>\n",
              "      <td>unknown</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Love_by_the_Ligh...</td>\n",
              "      <td>The moon, painted with a smiling face hangs ov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1901</td>\n",
              "      <td>The Martyred Presidents</td>\n",
              "      <td>American</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>NaN</td>\n",
              "      <td>unknown</td>\n",
              "      <td>https://en.wikipedia.org/wiki/The_Martyred_Pre...</td>\n",
              "      <td>The film, just over a minute long, is composed...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1901</td>\n",
              "      <td>Terrible Teddy, the Grizzly King</td>\n",
              "      <td>American</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>NaN</td>\n",
              "      <td>unknown</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Terrible_Teddy,_...</td>\n",
              "      <td>Lasting just 61 seconds and consisting of two ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1902</td>\n",
              "      <td>Jack and the Beanstalk</td>\n",
              "      <td>American</td>\n",
              "      <td>George S. Fleming, Edwin S. Porter</td>\n",
              "      <td>NaN</td>\n",
              "      <td>unknown</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Jack_and_the_Bea...</td>\n",
              "      <td>The earliest known adaptation of the classic f...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d62052b1-45bf-4574-b51a-793be2d7f774')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d62052b1-45bf-4574-b51a-793be2d7f774 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d62052b1-45bf-4574-b51a-793be2d7f774');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-bd533454-3eda-4ca9-9ecf-566070669932\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bd533454-3eda-4ca9-9ecf-566070669932')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-bd533454-3eda-4ca9-9ecf-566070669932 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   Release Year                             Title Origin/Ethnicity  \\\n",
              "0          1901            Kansas Saloon Smashers         American   \n",
              "1          1901     Love by the Light of the Moon         American   \n",
              "2          1901           The Martyred Presidents         American   \n",
              "3          1901  Terrible Teddy, the Grizzly King         American   \n",
              "4          1902            Jack and the Beanstalk         American   \n",
              "\n",
              "                             Director Cast    Genre  \\\n",
              "0                             Unknown  NaN  unknown   \n",
              "1                             Unknown  NaN  unknown   \n",
              "2                             Unknown  NaN  unknown   \n",
              "3                             Unknown  NaN  unknown   \n",
              "4  George S. Fleming, Edwin S. Porter  NaN  unknown   \n",
              "\n",
              "                                           Wiki Page  \\\n",
              "0  https://en.wikipedia.org/wiki/Kansas_Saloon_Sm...   \n",
              "1  https://en.wikipedia.org/wiki/Love_by_the_Ligh...   \n",
              "2  https://en.wikipedia.org/wiki/The_Martyred_Pre...   \n",
              "3  https://en.wikipedia.org/wiki/Terrible_Teddy,_...   \n",
              "4  https://en.wikipedia.org/wiki/Jack_and_the_Bea...   \n",
              "\n",
              "                                                Plot  \n",
              "0  A bartender is working at a saloon, serving dr...  \n",
              "1  The moon, painted with a smiling face hangs ov...  \n",
              "2  The film, just over a minute long, is composed...  \n",
              "3  Lasting just 61 seconds and consisting of two ...  \n",
              "4  The earliest known adaptation of the classic f...  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "lKCejlpdjwd7"
      },
      "outputs": [],
      "source": [
        "import math, time, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "YKUwJx7ylA48"
      },
      "outputs": [],
      "source": [
        "r_seed = 42\n",
        "torch.manual_seed(r_seed)\n",
        "np.random.seed(r_seed)\n",
        "random.seed(r_seed)\n",
        "\n",
        "data_path = 'movie.csv'\n",
        "text_col = 'Plot'\n",
        "batch_size = 32\n",
        "block_size = 64\n",
        "embed_dim = 128\n",
        "num_head = 8\n",
        "num_layers = 2\n",
        "ff_dim = 4 * embed_dim\n",
        "dropout = 0.1\n",
        "lr = 3e-4\n",
        "epochs = 10\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print_every = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9r1-XWeZmBDL",
        "outputId": "d2ff1d9a-0894-4c52-b318-c285fe712796"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "YXEmGfmNmDH7"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(data_path)\n",
        "texts = df[text_col].astype(str).tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyPK1fgkmUR3",
        "outputId": "912de5fa-114b-47e7-85d1-3a98131440db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\"A bartender is working at a saloon, serving drinks to customers. After he fills a stereotypically Irish man's bucket with beer, Carrie Nation and her followers burst inside. They assault the Irish man, pulling his hat over his eyes and then dumping the beer over his head. The group then begin wrecking the bar, smashing the fixtures, mirrors, and breaking the cash register. The bartender then sprays seltzer water in Nation's face before a group of policemen appear and order everybody to leave.[1]\", \"The moon, painted with a smiling face hangs over a park at night. A young couple walking past a fence learn on a railing and look up. The moon smiles. They embrace, and the moon's smile gets bigger. They then sit down on a bench by a tree. The moon's view is blocked, causing him to frown. In the last scene, the man fans the woman with his hat because the moon has left the sky and is perched over her shoulder to see everything better.\", 'The film, just over a minute long, is composed of two shots. In the first, a girl sits at the base of an altar or tomb, her face hidden from the camera. At the center of the altar, a viewing portal displays the portraits of three U.S. Presidents—Abraham Lincoln, James A. Garfield, and William McKinley—each victims of assassination.\\r\\nIn the second shot, which runs just over eight seconds long, an assassin kneels feet of Lady Justice.', 'Lasting just 61 seconds and consisting of two shots, the first shot is set in a wood during winter. The actor representing then vice-president Theodore Roosevelt enthusiastically hurries down a hillside towards a tree in the foreground. He falls once, but rights himself and cocks his rifle. Two other men, bearing signs reading \"His Photographer\" and \"His Press Agent\" respectively, follow him into the shot; the photographer sets up his camera. \"Teddy\" aims his rifle upward at the tree and fells what appears to be a common house cat, which he then proceeds to stab. \"Teddy\" holds his prize aloft, and the press agent takes notes. The second shot is taken in a slightly different part of the wood, on a path. \"Teddy\" rides the path on his horse towards the camera and out to the left of the shot, followed closely by the press agent and photographer, still dutifully holding their signs.', \"The earliest known adaptation of the classic fairytale, this films shows Jack trading his cow for the beans, his mother forcing him to drop them in the front yard, and beig forced upstairs. As he sleeps, Jack is visited by a fairy who shows him glimpses of what will await him when he ascends the bean stalk. In this version, Jack is the son of a deposed king. When Jack wakes up, he finds the beanstalk has grown and he climbs to the top where he enters the giant's home. The giant finds Jack, who narrowly escapes. The giant chases Jack down the bean stalk, but Jack is able to cut it down before the giant can get to safety. He falls and is killed as Jack celebrates. The fairy then reveals that Jack may return home as a prince.\"]\n"
          ]
        }
      ],
      "source": [
        "new = texts[: 5]\n",
        "print(new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "8g_Ep-oWmduJ"
      },
      "outputs": [],
      "source": [
        "tokenizer = get_tokenizer('basic_english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "4k3EiO-rmlTL"
      },
      "outputs": [],
      "source": [
        "def build_vocab(texts, min_freq = 1, max_vocab = None):\n",
        "  cnt = Counter()\n",
        "  for t in texts:\n",
        "    toks = tokenizer(t)\n",
        "    cnt.update(toks)\n",
        "  items = [w for w, f in cnt.most_common() if f >= min_freq]\n",
        "\n",
        "  if max_vocab:\n",
        "    items = items[:max_vocab]\n",
        "\n",
        "  itos = ['<pad>', '<unk>'] + items\n",
        "  stoi = {w: i for i, w in enumerate(itos)}\n",
        "  return stoi, itos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFE2iB-6nXVb",
        "outputId": "3a64eae5-bbc3-48e7-88ee-e7a2b4b1705a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 30002\n"
          ]
        }
      ],
      "source": [
        "stoi, itos = build_vocab(texts, min_freq = 1, max_vocab = 30000)\n",
        "vocab_size = len(itos)\n",
        "pad_idx = stoi['<pad>']\n",
        "unk_idx = stoi['<unk>']\n",
        "print(\"Vocab size:\", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXutUtq2nyhd",
        "outputId": "8eebcd19-e239-4d36-8229-4f6256867b96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens length :  15115524\n"
          ]
        }
      ],
      "source": [
        "all_tokens = []\n",
        "sep_token = '<sep>'\n",
        "\n",
        "if sep_token not in stoi:\n",
        "  stoi[sep_token] = len(itos)\n",
        "  itos.append(sep_token)\n",
        "  vocab_size += 1\n",
        "\n",
        "for t in texts:\n",
        "  toks = tokenizer(t)\n",
        "  ids = [stoi.get(tok, unk_idx) for tok in toks]\n",
        "  ids.append(stoi[sep_token])\n",
        "  all_tokens.extend(ids)\n",
        "\n",
        "print('Total tokens length : ', len(all_tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "bR66FofOovvr"
      },
      "outputs": [],
      "source": [
        "class TokenDataset(Dataset):\n",
        "    def __init__(self, token_list, block_size, stride=None):\n",
        "        self.tokens = token_list\n",
        "        self.block_size = block_size\n",
        "        self.stride = stride if stride is not None else block_size\n",
        "\n",
        "        # compute valid start positions\n",
        "        self.starts = list(range(0, max(0, len(self.tokens) - self.block_size + 1), self.stride))\n",
        "        self.num_blocks = len(self.starts)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_blocks\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        idx = self.starts[i]\n",
        "        x = torch.tensor(self.tokens[idx : idx + self.block_size], dtype=torch.long)\n",
        "        y = torch.tensor(self.tokens[idx + 1 : idx + 1 + self.block_size], dtype=torch.long)\n",
        "        return x, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e2KPzrytZwl",
        "outputId": "762d8422-c52b-4fa0-ac91-f3596085f7cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train blocks : 233818 Val blocks : 2362\n"
          ]
        }
      ],
      "source": [
        "dataset = TokenDataset(all_tokens, block_size=64, stride=64)\n",
        "train_size = int(0.99 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_ds, val_ds = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "train_loader = DataLoader(train_ds, batch_size = batch_size, shuffle = True, drop_last = True)\n",
        "val_loader = DataLoader(val_ds, batch_size = batch_size, shuffle = False, drop_last = True)\n",
        "\n",
        "print(\"Train blocks :\", len(train_ds), \"Val blocks :\", len(val_ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "4HWB7a8fupwG"
      },
      "outputs": [],
      "source": [
        "class CasualMultiHeadSelfAttention(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads, dropout = 0.0):\n",
        "    super().__init__()\n",
        "    assert embed_dim % num_heads == 0\n",
        "    self.embed_dim = embed_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = embed_dim // num_heads\n",
        "\n",
        "    self.W_q = nn.Linear(embed_dim, embed_dim)\n",
        "    self.W_k = nn.Linear(embed_dim, embed_dim)\n",
        "    self.W_v = nn.Linear(embed_dim, embed_dim)\n",
        "    self.out = nn.Linear(embed_dim, embed_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, attn_mask = None):\n",
        "    B, L, D = x.shape\n",
        "    Q = self.W_q(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
        "    K = self.W_k(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
        "    V = self.W_v(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
        "\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "    if attn_mask is None:\n",
        "      mask = torch.triu(torch.ones(L, L, device = x.device),diagonal = 1).bool()\n",
        "\n",
        "      scores = scores.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
        "\n",
        "    else:\n",
        "      scores = scores.masked_fill(attn_mask.unsqueeze(0).unsquueze(0), float('-inf'))\n",
        "\n",
        "\n",
        "    attn = torch.softmax(scores, dim = -1)\n",
        "    attn = self.dropout(attn)\n",
        "    out = torch.matmul(attn, V)\n",
        "    out = out.transpose(1, 2).contiguous().view(B, L, D)\n",
        "    out = self.out(out)\n",
        "    return out, attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "VU_cKYgUxbSq"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads, ff_dim, dropout = 0.1):\n",
        "    super().__init__()\n",
        "    self.attn = CasualMultiHeadSelfAttention(embed_dim, num_heads, dropout = dropout)\n",
        "    self.ln1 = nn.LayerNorm(embed_dim)\n",
        "    self.ff = nn.Sequential(\n",
        "        nn.Linear(embed_dim, ff_dim),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(ff_dim, embed_dim),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "    self.ln2 = nn.LayerNorm(embed_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    attn_out, attn_weights = self.attn(x)\n",
        "    x = x + self.dropout(attn_out)\n",
        "    x = self.ln1(x)\n",
        "    ff_out = self.ff(x)\n",
        "    x = x + self.dropout(ff_out)\n",
        "    x = self.ln2(x)\n",
        "    return x, attn_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "1zVqInUiy7ez"
      },
      "outputs": [],
      "source": [
        "class TinyGpt(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim, num_heads, num_layers, ff_dim, block_size, dropout = 0.1, pad_idx = 0):\n",
        "    super().__init__()\n",
        "    self.token_emb = nn.Embedding(vocab_size, embed_dim, padding_idx = pad_idx)\n",
        "    self.pos_emb = nn.Embedding(block_size, embed_dim)\n",
        "    self.blocks = nn.ModuleList([DecoderBlock(embed_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)])\n",
        "    self.ln_f = nn.LayerNorm(embed_dim)\n",
        "    self.head = nn.Linear(embed_dim, vocab_size, bias = False)\n",
        "    self.block_size = block_size\n",
        "\n",
        "  def forward(self, idx):\n",
        "\n",
        "    B, L = idx.shape\n",
        "    assert L <= self.block_size\n",
        "    token_embeddings = self.token_emb(idx)\n",
        "    pos_ids = torch.arange(0, L, dtype = torch.long, device = idx.device).unsqueeze(0)\n",
        "    pos_embeddings = self.pos_emb(pos_ids)\n",
        "    x = token_embeddings + pos_embeddings\n",
        "    attn_maps = []\n",
        "    for block in self.blocks:\n",
        "      x, attn = block(x)\n",
        "      attn_maps.append(attn)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.head(x)\n",
        "    return logits, attn_maps\n",
        "\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def generate(self, idx, max_new_tokens, temperature = 1.0, top_k = None):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -self.block_size:]\n",
        "      logits, _ = self.forward(idx_cond)\n",
        "      logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "      if top_k is not None:\n",
        "        v, _ = torch.topk(logits, top_k)\n",
        "        min_v = v[:, -1].unsqueeze(-1)\n",
        "        logits = torch.where(logits < min_v, torch.full_like(logits, -float('Inf')), logits)\n",
        "      probs = F.softmax(logits, dim = -1)\n",
        "      next_token = torch.multinomial(probs, num_samples = 1)\n",
        "      idx = torch.cat([idx, next_token], dim = 1)\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tczsYUtX2hWg",
        "outputId": "ebcd4bf7-682b-4676-a642-eb78be67fce2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model params : 8.08576 M\n"
          ]
        }
      ],
      "source": [
        "model = TinyGpt(vocab_size = vocab_size, embed_dim = embed_dim, num_heads = num_head,\n",
        "                num_layers = num_layers, ff_dim = ff_dim, block_size = block_size, dropout = dropout,\n",
        "                pad_idx = pad_idx).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = lr, weight_decay=1e-2)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"Model params :\", sum(p.numel() for p in model.parameters()) / 1e6, \"M\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xitPGxAY30qi",
        "outputId": "d1c1c18b-5613-43c9-982e-efd626c6e39b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Step 200 | train_loss 7.4280 | val loss 6.5610\n",
            "Epoch 1 | Step 400 | train_loss 6.4290 | val loss 6.2586\n",
            "Epoch 1 | Step 600 | train_loss 6.1976 | val loss 6.0737\n",
            "Epoch 1 | Step 800 | train_loss 6.0410 | val loss 5.9491\n",
            "Epoch 1 | Step 1000 | train_loss 5.9258 | val loss 5.8595\n",
            "Epoch 1 | Step 1200 | train_loss 5.8585 | val loss 5.7877\n",
            "Epoch 1 | Step 1400 | train_loss 5.8000 | val loss 5.7335\n",
            "Epoch 1 | Step 1600 | train_loss 5.7480 | val loss 5.6863\n",
            "Epoch 1 | Step 1800 | train_loss 5.7113 | val loss 5.6455\n",
            "Epoch 1 | Step 2000 | train_loss 5.6748 | val loss 5.6101\n",
            "Epoch 1 | Step 2200 | train_loss 5.6466 | val loss 5.5783\n",
            "Epoch 1 | Step 2400 | train_loss 5.6098 | val loss 5.5463\n",
            "Epoch 1 | Step 2600 | train_loss 5.5804 | val loss 5.5191\n",
            "Epoch 1 | Step 2800 | train_loss 5.5659 | val loss 5.4946\n",
            "Epoch 1 | Step 3000 | train_loss 5.5384 | val loss 5.4715\n",
            "Epoch 1 | Step 3200 | train_loss 5.5217 | val loss 5.4501\n",
            "Epoch 1 | Step 3400 | train_loss 5.5009 | val loss 5.4301\n",
            "Epoch 1 | Step 3600 | train_loss 5.4827 | val loss 5.4093\n",
            "Epoch 1 | Step 3800 | train_loss 5.4716 | val loss 5.3946\n",
            "Epoch 1 | Step 4000 | train_loss 5.4396 | val loss 5.3752\n",
            "Epoch 1 | Step 4200 | train_loss 5.4395 | val loss 5.3567\n",
            "Epoch 1 | Step 4400 | train_loss 5.4145 | val loss 5.3436\n",
            "Epoch 1 | Step 4600 | train_loss 5.3975 | val loss 5.3277\n",
            "Epoch 1 | Step 4800 | train_loss 5.3825 | val loss 5.3154\n",
            "Epoch 1 | Step 5000 | train_loss 5.3784 | val loss 5.3019\n",
            "Epoch 1 | Step 5200 | train_loss 5.3639 | val loss 5.2868\n",
            "Epoch 1 | Step 5400 | train_loss 5.3444 | val loss 5.2766\n",
            "Epoch 1 | Step 5600 | train_loss 5.3340 | val loss 5.2662\n",
            "Epoch 1 | Step 5800 | train_loss 5.3150 | val loss 5.2517\n",
            "Epoch 1 | Step 6000 | train_loss 5.3077 | val loss 5.2405\n",
            "Epoch 1 | Step 6200 | train_loss 5.3089 | val loss 5.2292\n",
            "Epoch 1 | Step 6400 | train_loss 5.2890 | val loss 5.2168\n",
            "Epoch 1 | Step 6600 | train_loss 5.2791 | val loss 5.2060\n",
            "Epoch 1 | Step 6800 | train_loss 5.2741 | val loss 5.1988\n",
            "Epoch 1 | Step 7000 | train_loss 5.2709 | val loss 5.1876\n",
            "Epoch 1 | Step 7200 | train_loss 5.2462 | val loss 5.1767\n",
            "Epoch 1 finished in 239.5s | val loss = 5.1723\n",
            "Saved best model.\n",
            "Epoch 2 | Step 7400 | train_loss 2.4490 | val loss 5.1686\n",
            "Epoch 2 | Step 7600 | train_loss 5.2052 | val loss 5.1614\n",
            "Epoch 2 | Step 7800 | train_loss 5.2017 | val loss 5.1542\n",
            "Epoch 2 | Step 8000 | train_loss 5.1927 | val loss 5.1460\n",
            "Epoch 2 | Step 8200 | train_loss 5.1743 | val loss 5.1383\n",
            "Epoch 2 | Step 8400 | train_loss 5.1834 | val loss 5.1309\n",
            "Epoch 2 | Step 8600 | train_loss 5.1645 | val loss 5.1239\n",
            "Epoch 2 | Step 8800 | train_loss 5.1603 | val loss 5.1165\n",
            "Epoch 2 | Step 9000 | train_loss 5.1605 | val loss 5.1082\n",
            "Epoch 2 | Step 9200 | train_loss 5.1664 | val loss 5.1000\n",
            "Epoch 2 | Step 9400 | train_loss 5.1475 | val loss 5.0947\n",
            "Epoch 2 | Step 9600 | train_loss 5.1395 | val loss 5.0863\n",
            "Epoch 2 | Step 9800 | train_loss 5.1381 | val loss 5.0796\n",
            "Epoch 2 | Step 10000 | train_loss 5.1372 | val loss 5.0737\n",
            "Epoch 2 | Step 10200 | train_loss 5.1232 | val loss 5.0660\n",
            "Epoch 2 | Step 10400 | train_loss 5.1098 | val loss 5.0596\n",
            "Epoch 2 | Step 10600 | train_loss 5.1130 | val loss 5.0530\n",
            "Epoch 2 | Step 10800 | train_loss 5.1113 | val loss 5.0461\n",
            "Epoch 2 | Step 11000 | train_loss 5.1055 | val loss 5.0410\n",
            "Epoch 2 | Step 11200 | train_loss 5.0923 | val loss 5.0345\n",
            "Epoch 2 | Step 11400 | train_loss 5.0926 | val loss 5.0273\n",
            "Epoch 2 | Step 11600 | train_loss 5.0960 | val loss 5.0215\n",
            "Epoch 2 | Step 11800 | train_loss 5.0765 | val loss 5.0181\n",
            "Epoch 2 | Step 12000 | train_loss 5.0796 | val loss 5.0105\n",
            "Epoch 2 | Step 12200 | train_loss 5.0655 | val loss 5.0043\n",
            "Epoch 2 | Step 12400 | train_loss 5.0630 | val loss 4.9980\n",
            "Epoch 2 | Step 12600 | train_loss 5.0499 | val loss 4.9938\n",
            "Epoch 2 | Step 12800 | train_loss 5.0528 | val loss 4.9875\n",
            "Epoch 2 | Step 13000 | train_loss 5.0541 | val loss 4.9834\n",
            "Epoch 2 | Step 13200 | train_loss 5.0407 | val loss 4.9785\n",
            "Epoch 2 | Step 13400 | train_loss 5.0447 | val loss 4.9717\n",
            "Epoch 2 | Step 13600 | train_loss 5.0388 | val loss 4.9678\n",
            "Epoch 2 | Step 13800 | train_loss 5.0293 | val loss 4.9624\n",
            "Epoch 2 | Step 14000 | train_loss 5.0206 | val loss 4.9549\n",
            "Epoch 2 | Step 14200 | train_loss 5.0254 | val loss 4.9503\n",
            "Epoch 2 | Step 14400 | train_loss 5.0154 | val loss 4.9462\n",
            "Epoch 2 | Step 14600 | train_loss 5.0203 | val loss 4.9432\n",
            "Epoch 2 finished in 241.6s | val loss = 4.9407\n",
            "Saved best model.\n",
            "Epoch 3 | Step 14800 | train_loss 4.6728 | val loss 4.9414\n",
            "Epoch 3 | Step 15000 | train_loss 4.9615 | val loss 4.9365\n",
            "Epoch 3 | Step 15200 | train_loss 4.9533 | val loss 4.9326\n",
            "Epoch 3 | Step 15400 | train_loss 4.9586 | val loss 4.9280\n",
            "Epoch 3 | Step 15600 | train_loss 4.9533 | val loss 4.9235\n",
            "Epoch 3 | Step 15800 | train_loss 4.9648 | val loss 4.9216\n",
            "Epoch 3 | Step 16000 | train_loss 4.9528 | val loss 4.9178\n",
            "Epoch 3 | Step 16200 | train_loss 4.9530 | val loss 4.9130\n",
            "Epoch 3 | Step 16400 | train_loss 4.9522 | val loss 4.9085\n",
            "Epoch 3 | Step 16600 | train_loss 4.9418 | val loss 4.9030\n",
            "Epoch 3 | Step 16800 | train_loss 4.9453 | val loss 4.9022\n",
            "Epoch 3 | Step 17000 | train_loss 4.9447 | val loss 4.8956\n",
            "Epoch 3 | Step 17200 | train_loss 4.9325 | val loss 4.8937\n",
            "Epoch 3 | Step 17400 | train_loss 4.9501 | val loss 4.8906\n",
            "Epoch 3 | Step 17600 | train_loss 4.9275 | val loss 4.8851\n",
            "Epoch 3 | Step 17800 | train_loss 4.9192 | val loss 4.8833\n",
            "Epoch 3 | Step 18000 | train_loss 4.9296 | val loss 4.8778\n",
            "Epoch 3 | Step 18200 | train_loss 4.9165 | val loss 4.8739\n",
            "Epoch 3 | Step 18400 | train_loss 4.9189 | val loss 4.8697\n",
            "Epoch 3 | Step 18600 | train_loss 4.9156 | val loss 4.8667\n",
            "Epoch 3 | Step 18800 | train_loss 4.9132 | val loss 4.8614\n",
            "Epoch 3 | Step 19000 | train_loss 4.9187 | val loss 4.8592\n",
            "Epoch 3 | Step 19200 | train_loss 4.9049 | val loss 4.8553\n",
            "Epoch 3 | Step 19400 | train_loss 4.9091 | val loss 4.8511\n",
            "Epoch 3 | Step 19600 | train_loss 4.9029 | val loss 4.8487\n",
            "Epoch 3 | Step 19800 | train_loss 4.8936 | val loss 4.8441\n",
            "Epoch 3 | Step 20000 | train_loss 4.8987 | val loss 4.8395\n",
            "Epoch 3 | Step 20200 | train_loss 4.8877 | val loss 4.8366\n",
            "Epoch 3 | Step 20400 | train_loss 4.8973 | val loss 4.8336\n",
            "Epoch 3 | Step 20600 | train_loss 4.8906 | val loss 4.8306\n",
            "Epoch 3 | Step 20800 | train_loss 4.8962 | val loss 4.8264\n",
            "Epoch 3 | Step 21000 | train_loss 4.8804 | val loss 4.8234\n",
            "Epoch 3 | Step 21200 | train_loss 4.8700 | val loss 4.8191\n",
            "Epoch 3 | Step 21400 | train_loss 4.8805 | val loss 4.8190\n",
            "Epoch 3 | Step 21600 | train_loss 4.8692 | val loss 4.8155\n",
            "Epoch 3 | Step 21800 | train_loss 4.8656 | val loss 4.8109\n",
            "Epoch 3 finished in 240.9s | val loss = 4.8110\n",
            "Saved best model.\n",
            "Epoch 4 | Step 22000 | train_loss 1.9771 | val loss 4.8100\n",
            "Epoch 4 | Step 22200 | train_loss 4.8107 | val loss 4.8091\n",
            "Epoch 4 | Step 22400 | train_loss 4.8298 | val loss 4.8071\n",
            "Epoch 4 | Step 22600 | train_loss 4.8168 | val loss 4.8049\n",
            "Epoch 4 | Step 22800 | train_loss 4.8062 | val loss 4.8025\n",
            "Epoch 4 | Step 23000 | train_loss 4.8322 | val loss 4.7998\n",
            "Epoch 4 | Step 23200 | train_loss 4.8113 | val loss 4.7984\n",
            "Epoch 4 | Step 23400 | train_loss 4.8182 | val loss 4.7955\n",
            "Epoch 4 | Step 23600 | train_loss 4.8156 | val loss 4.7921\n",
            "Epoch 4 | Step 23800 | train_loss 4.8064 | val loss 4.7896\n",
            "Epoch 4 | Step 24000 | train_loss 4.8084 | val loss 4.7873\n",
            "Epoch 4 | Step 24200 | train_loss 4.8162 | val loss 4.7852\n",
            "Epoch 4 | Step 24400 | train_loss 4.8171 | val loss 4.7819\n",
            "Epoch 4 | Step 24600 | train_loss 4.8055 | val loss 4.7803\n",
            "Epoch 4 | Step 24800 | train_loss 4.8080 | val loss 4.7757\n",
            "Epoch 4 | Step 25000 | train_loss 4.8093 | val loss 4.7738\n",
            "Epoch 4 | Step 25200 | train_loss 4.8044 | val loss 4.7719\n",
            "Epoch 4 | Step 25400 | train_loss 4.8041 | val loss 4.7691\n",
            "Epoch 4 | Step 25600 | train_loss 4.8026 | val loss 4.7674\n",
            "Epoch 4 | Step 25800 | train_loss 4.8047 | val loss 4.7650\n",
            "Epoch 4 | Step 26000 | train_loss 4.7976 | val loss 4.7639\n",
            "Epoch 4 | Step 26200 | train_loss 4.7950 | val loss 4.7613\n",
            "Epoch 4 | Step 26400 | train_loss 4.8029 | val loss 4.7604\n",
            "Epoch 4 | Step 26600 | train_loss 4.7997 | val loss 4.7568\n",
            "Epoch 4 | Step 26800 | train_loss 4.8001 | val loss 4.7541\n",
            "Epoch 4 | Step 27000 | train_loss 4.7859 | val loss 4.7525\n",
            "Epoch 4 | Step 27200 | train_loss 4.8036 | val loss 4.7488\n",
            "Epoch 4 | Step 27400 | train_loss 4.7955 | val loss 4.7487\n",
            "Epoch 4 | Step 27600 | train_loss 4.7808 | val loss 4.7453\n",
            "Epoch 4 | Step 27800 | train_loss 4.7881 | val loss 4.7422\n",
            "Epoch 4 | Step 28000 | train_loss 4.7796 | val loss 4.7429\n",
            "Epoch 4 | Step 28200 | train_loss 4.7934 | val loss 4.7386\n",
            "Epoch 4 | Step 28400 | train_loss 4.7827 | val loss 4.7364\n",
            "Epoch 4 | Step 28600 | train_loss 4.7820 | val loss 4.7349\n",
            "Epoch 4 | Step 28800 | train_loss 4.7751 | val loss 4.7325\n",
            "Epoch 4 | Step 29000 | train_loss 4.7845 | val loss 4.7292\n",
            "Epoch 4 | Step 29200 | train_loss 4.7811 | val loss 4.7270\n",
            "Epoch 4 finished in 240.6s | val loss = 4.7272\n",
            "Saved best model.\n",
            "Epoch 5 | Step 29400 | train_loss 4.1520 | val loss 4.7294\n",
            "Epoch 5 | Step 29600 | train_loss 4.7283 | val loss 4.7265\n",
            "Epoch 5 | Step 29800 | train_loss 4.7301 | val loss 4.7283\n",
            "Epoch 5 | Step 30000 | train_loss 4.7164 | val loss 4.7248\n",
            "Epoch 5 | Step 30200 | train_loss 4.7241 | val loss 4.7230\n",
            "Epoch 5 | Step 30400 | train_loss 4.7223 | val loss 4.7210\n",
            "Epoch 5 | Step 30600 | train_loss 4.7286 | val loss 4.7188\n",
            "Epoch 5 | Step 30800 | train_loss 4.7344 | val loss 4.7169\n",
            "Epoch 5 | Step 31000 | train_loss 4.7270 | val loss 4.7165\n",
            "Epoch 5 | Step 31200 | train_loss 4.7237 | val loss 4.7157\n",
            "Epoch 5 | Step 31400 | train_loss 4.7297 | val loss 4.7140\n",
            "Epoch 5 | Step 31600 | train_loss 4.7373 | val loss 4.7115\n",
            "Epoch 5 | Step 31800 | train_loss 4.7217 | val loss 4.7114\n",
            "Epoch 5 | Step 32000 | train_loss 4.7306 | val loss 4.7076\n",
            "Epoch 5 | Step 32200 | train_loss 4.7261 | val loss 4.7070\n",
            "Epoch 5 | Step 32400 | train_loss 4.7244 | val loss 4.7053\n",
            "Epoch 5 | Step 32600 | train_loss 4.7258 | val loss 4.7028\n",
            "Epoch 5 | Step 32800 | train_loss 4.7322 | val loss 4.6997\n",
            "Epoch 5 | Step 33000 | train_loss 4.7199 | val loss 4.7004\n",
            "Epoch 5 | Step 33200 | train_loss 4.7231 | val loss 4.6964\n",
            "Epoch 5 | Step 33400 | train_loss 4.7087 | val loss 4.6970\n",
            "Epoch 5 | Step 33600 | train_loss 4.7212 | val loss 4.6951\n",
            "Epoch 5 | Step 33800 | train_loss 4.7233 | val loss 4.6940\n",
            "Epoch 5 | Step 34000 | train_loss 4.7099 | val loss 4.6904\n",
            "Epoch 5 | Step 34200 | train_loss 4.7275 | val loss 4.6899\n",
            "Epoch 5 | Step 34400 | train_loss 4.7212 | val loss 4.6874\n",
            "Epoch 5 | Step 34600 | train_loss 4.7190 | val loss 4.6845\n",
            "Epoch 5 | Step 34800 | train_loss 4.6974 | val loss 4.6856\n",
            "Epoch 5 | Step 35000 | train_loss 4.7180 | val loss 4.6840\n",
            "Epoch 5 | Step 35200 | train_loss 4.7119 | val loss 4.6807\n",
            "Epoch 5 | Step 35400 | train_loss 4.7101 | val loss 4.6810\n",
            "Epoch 5 | Step 35600 | train_loss 4.7080 | val loss 4.6781\n",
            "Epoch 5 | Step 35800 | train_loss 4.7040 | val loss 4.6768\n",
            "Epoch 5 | Step 36000 | train_loss 4.7096 | val loss 4.6757\n",
            "Epoch 5 | Step 36200 | train_loss 4.7080 | val loss 4.6722\n",
            "Epoch 5 | Step 36400 | train_loss 4.7139 | val loss 4.6722\n",
            "Epoch 5 finished in 240.5s | val loss = 4.6696\n",
            "Saved best model.\n",
            "Epoch 6 | Step 36600 | train_loss 1.6255 | val loss 4.6719\n",
            "Epoch 6 | Step 36800 | train_loss 4.6560 | val loss 4.6728\n",
            "Epoch 6 | Step 37000 | train_loss 4.6526 | val loss 4.6692\n",
            "Epoch 6 | Step 37200 | train_loss 4.6567 | val loss 4.6688\n",
            "Epoch 6 | Step 37400 | train_loss 4.6582 | val loss 4.6696\n",
            "Epoch 6 | Step 37600 | train_loss 4.6596 | val loss 4.6689\n",
            "Epoch 6 | Step 37800 | train_loss 4.6614 | val loss 4.6682\n",
            "Epoch 6 | Step 38000 | train_loss 4.6556 | val loss 4.6654\n",
            "Epoch 6 | Step 38200 | train_loss 4.6598 | val loss 4.6673\n",
            "Epoch 6 | Step 38400 | train_loss 4.6678 | val loss 4.6621\n",
            "Epoch 6 | Step 38600 | train_loss 4.6604 | val loss 4.6611\n",
            "Epoch 6 | Step 38800 | train_loss 4.6612 | val loss 4.6614\n",
            "Epoch 6 | Step 39000 | train_loss 4.6563 | val loss 4.6585\n",
            "Epoch 6 | Step 39200 | train_loss 4.6727 | val loss 4.6562\n",
            "Epoch 6 | Step 39400 | train_loss 4.6632 | val loss 4.6570\n",
            "Epoch 6 | Step 39600 | train_loss 4.6548 | val loss 4.6561\n",
            "Epoch 6 | Step 39800 | train_loss 4.6538 | val loss 4.6533\n",
            "Epoch 6 | Step 40000 | train_loss 4.6639 | val loss 4.6506\n",
            "Epoch 6 | Step 40200 | train_loss 4.6622 | val loss 4.6489\n",
            "Epoch 6 | Step 40400 | train_loss 4.6525 | val loss 4.6480\n",
            "Epoch 6 | Step 40600 | train_loss 4.6588 | val loss 4.6479\n",
            "Epoch 6 | Step 40800 | train_loss 4.6661 | val loss 4.6459\n",
            "Epoch 6 | Step 41000 | train_loss 4.6627 | val loss 4.6429\n",
            "Epoch 6 | Step 41200 | train_loss 4.6577 | val loss 4.6439\n",
            "Epoch 6 | Step 41400 | train_loss 4.6497 | val loss 4.6417\n",
            "Epoch 6 | Step 41600 | train_loss 4.6538 | val loss 4.6409\n",
            "Epoch 6 | Step 41800 | train_loss 4.6610 | val loss 4.6375\n",
            "Epoch 6 | Step 42000 | train_loss 4.6546 | val loss 4.6364\n",
            "Epoch 6 | Step 42200 | train_loss 4.6679 | val loss 4.6332\n",
            "Epoch 6 | Step 42400 | train_loss 4.6458 | val loss 4.6343\n",
            "Epoch 6 | Step 42600 | train_loss 4.6637 | val loss 4.6328\n",
            "Epoch 6 | Step 42800 | train_loss 4.6500 | val loss 4.6299\n",
            "Epoch 6 | Step 43000 | train_loss 4.6585 | val loss 4.6309\n",
            "Epoch 6 | Step 43200 | train_loss 4.6632 | val loss 4.6280\n",
            "Epoch 6 | Step 43400 | train_loss 4.6509 | val loss 4.6278\n",
            "Epoch 6 | Step 43600 | train_loss 4.6582 | val loss 4.6262\n",
            "Epoch 6 | Step 43800 | train_loss 4.6535 | val loss 4.6259\n",
            "Epoch 6 finished in 241.4s | val loss = 4.6268\n",
            "Saved best model.\n",
            "Epoch 7 | Step 44000 | train_loss 3.7640 | val loss 4.6265\n",
            "Epoch 7 | Step 44200 | train_loss 4.6023 | val loss 4.6263\n",
            "Epoch 7 | Step 44400 | train_loss 4.5766 | val loss 4.6269\n",
            "Epoch 7 | Step 44600 | train_loss 4.6155 | val loss 4.6236\n",
            "Epoch 7 | Step 44800 | train_loss 4.6031 | val loss 4.6233\n",
            "Epoch 7 | Step 45000 | train_loss 4.6055 | val loss 4.6235\n",
            "Epoch 7 | Step 45200 | train_loss 4.6044 | val loss 4.6236\n",
            "Epoch 7 | Step 45400 | train_loss 4.6109 | val loss 4.6200\n",
            "Epoch 7 | Step 45600 | train_loss 4.6087 | val loss 4.6207\n",
            "Epoch 7 | Step 45800 | train_loss 4.6132 | val loss 4.6206\n",
            "Epoch 7 | Step 46000 | train_loss 4.6225 | val loss 4.6191\n",
            "Epoch 7 | Step 46200 | train_loss 4.6070 | val loss 4.6179\n",
            "Epoch 7 | Step 46400 | train_loss 4.6014 | val loss 4.6172\n",
            "Epoch 7 | Step 46600 | train_loss 4.6041 | val loss 4.6167\n",
            "Epoch 7 | Step 46800 | train_loss 4.5975 | val loss 4.6152\n",
            "Epoch 7 | Step 47000 | train_loss 4.6156 | val loss 4.6124\n",
            "Epoch 7 | Step 47200 | train_loss 4.6139 | val loss 4.6107\n",
            "Epoch 7 | Step 47400 | train_loss 4.6131 | val loss 4.6115\n",
            "Epoch 7 | Step 47600 | train_loss 4.6014 | val loss 4.6098\n",
            "Epoch 7 | Step 47800 | train_loss 4.6036 | val loss 4.6076\n",
            "Epoch 7 | Step 48000 | train_loss 4.5996 | val loss 4.6089\n",
            "Epoch 7 | Step 48200 | train_loss 4.6271 | val loss 4.6072\n",
            "Epoch 7 | Step 48400 | train_loss 4.6170 | val loss 4.6051\n",
            "Epoch 7 | Step 48600 | train_loss 4.6065 | val loss 4.6031\n",
            "Epoch 7 | Step 48800 | train_loss 4.6171 | val loss 4.6027\n",
            "Epoch 7 | Step 49000 | train_loss 4.6151 | val loss 4.6029\n",
            "Epoch 7 | Step 49200 | train_loss 4.6037 | val loss 4.5998\n",
            "Epoch 7 | Step 49400 | train_loss 4.6042 | val loss 4.5970\n",
            "Epoch 7 | Step 49600 | train_loss 4.6079 | val loss 4.5974\n",
            "Epoch 7 | Step 49800 | train_loss 4.6149 | val loss 4.5966\n",
            "Epoch 7 | Step 50000 | train_loss 4.6177 | val loss 4.5950\n",
            "Epoch 7 | Step 50200 | train_loss 4.6082 | val loss 4.5943\n",
            "Epoch 7 | Step 50400 | train_loss 4.6269 | val loss 4.5942\n",
            "Epoch 7 | Step 50600 | train_loss 4.6101 | val loss 4.5915\n",
            "Epoch 7 | Step 50800 | train_loss 4.6013 | val loss 4.5912\n",
            "Epoch 7 | Step 51000 | train_loss 4.6054 | val loss 4.5883\n",
            "Epoch 7 finished in 241.6s | val loss = 4.5886\n",
            "Saved best model.\n",
            "Epoch 8 | Step 51200 | train_loss 1.3210 | val loss 4.5906\n",
            "Epoch 8 | Step 51400 | train_loss 4.5462 | val loss 4.5899\n",
            "Epoch 8 | Step 51600 | train_loss 4.5529 | val loss 4.5895\n",
            "Epoch 8 | Step 51800 | train_loss 4.5462 | val loss 4.5917\n",
            "Epoch 8 | Step 52000 | train_loss 4.5712 | val loss 4.5901\n",
            "Epoch 8 | Step 52200 | train_loss 4.5608 | val loss 4.5903\n",
            "Epoch 8 | Step 52400 | train_loss 4.5613 | val loss 4.5891\n",
            "Epoch 8 | Step 52600 | train_loss 4.5586 | val loss 4.5877\n",
            "Epoch 8 | Step 52800 | train_loss 4.5585 | val loss 4.5876\n",
            "Epoch 8 | Step 53000 | train_loss 4.5585 | val loss 4.5852\n",
            "Epoch 8 | Step 53200 | train_loss 4.5602 | val loss 4.5859\n",
            "Epoch 8 | Step 53400 | train_loss 4.5628 | val loss 4.5850\n",
            "Epoch 8 | Step 53600 | train_loss 4.5647 | val loss 4.5826\n",
            "Epoch 8 | Step 53800 | train_loss 4.5693 | val loss 4.5806\n",
            "Epoch 8 | Step 54000 | train_loss 4.5637 | val loss 4.5798\n",
            "Epoch 8 | Step 54200 | train_loss 4.5757 | val loss 4.5802\n",
            "Epoch 8 | Step 54400 | train_loss 4.5754 | val loss 4.5769\n",
            "Epoch 8 | Step 54600 | train_loss 4.5735 | val loss 4.5773\n",
            "Epoch 8 | Step 54800 | train_loss 4.5685 | val loss 4.5781\n",
            "Epoch 8 | Step 55000 | train_loss 4.5684 | val loss 4.5763\n",
            "Epoch 8 | Step 55200 | train_loss 4.5739 | val loss 4.5763\n",
            "Epoch 8 | Step 55400 | train_loss 4.5744 | val loss 4.5733\n",
            "Epoch 8 | Step 55600 | train_loss 4.5691 | val loss 4.5712\n",
            "Epoch 8 | Step 55800 | train_loss 4.5785 | val loss 4.5715\n",
            "Epoch 8 | Step 56000 | train_loss 4.5689 | val loss 4.5693\n",
            "Epoch 8 | Step 56200 | train_loss 4.5760 | val loss 4.5692\n",
            "Epoch 8 | Step 56400 | train_loss 4.5720 | val loss 4.5663\n",
            "Epoch 8 | Step 56600 | train_loss 4.5694 | val loss 4.5660\n",
            "Epoch 8 | Step 56800 | train_loss 4.5721 | val loss 4.5653\n",
            "Epoch 8 | Step 57000 | train_loss 4.5645 | val loss 4.5647\n",
            "Epoch 8 | Step 57200 | train_loss 4.5710 | val loss 4.5636\n",
            "Epoch 8 | Step 57400 | train_loss 4.5695 | val loss 4.5622\n",
            "Epoch 8 | Step 57600 | train_loss 4.5584 | val loss 4.5634\n",
            "Epoch 8 | Step 57800 | train_loss 4.5677 | val loss 4.5601\n",
            "Epoch 8 | Step 58000 | train_loss 4.5744 | val loss 4.5600\n",
            "Epoch 8 | Step 58200 | train_loss 4.5725 | val loss 4.5596\n",
            "Epoch 8 | Step 58400 | train_loss 4.5643 | val loss 4.5578\n",
            "Epoch 8 finished in 241.7s | val loss = 4.5590\n",
            "Saved best model.\n",
            "Epoch 9 | Step 58600 | train_loss 3.4194 | val loss 4.5624\n",
            "Epoch 9 | Step 58800 | train_loss 4.5117 | val loss 4.5610\n",
            "Epoch 9 | Step 59000 | train_loss 4.5168 | val loss 4.5609\n",
            "Epoch 9 | Step 59200 | train_loss 4.5145 | val loss 4.5584\n",
            "Epoch 9 | Step 59400 | train_loss 4.5197 | val loss 4.5595\n",
            "Epoch 9 | Step 59600 | train_loss 4.5254 | val loss 4.5578\n",
            "Epoch 9 | Step 59800 | train_loss 4.5222 | val loss 4.5584\n",
            "Epoch 9 | Step 60000 | train_loss 4.5306 | val loss 4.5568\n",
            "Epoch 9 | Step 60200 | train_loss 4.5282 | val loss 4.5584\n",
            "Epoch 9 | Step 60400 | train_loss 4.5248 | val loss 4.5568\n",
            "Epoch 9 | Step 60600 | train_loss 4.5155 | val loss 4.5567\n",
            "Epoch 9 | Step 60800 | train_loss 4.5289 | val loss 4.5536\n",
            "Epoch 9 | Step 61000 | train_loss 4.5289 | val loss 4.5530\n",
            "Epoch 9 | Step 61200 | train_loss 4.5370 | val loss 4.5532\n",
            "Epoch 9 | Step 61400 | train_loss 4.5379 | val loss 4.5525\n",
            "Epoch 9 | Step 61600 | train_loss 4.5346 | val loss 4.5522\n",
            "Epoch 9 | Step 61800 | train_loss 4.5276 | val loss 4.5504\n",
            "Epoch 9 | Step 62000 | train_loss 4.5321 | val loss 4.5483\n",
            "Epoch 9 | Step 62200 | train_loss 4.5271 | val loss 4.5481\n",
            "Epoch 9 | Step 62400 | train_loss 4.5343 | val loss 4.5466\n",
            "Epoch 9 | Step 62600 | train_loss 4.5273 | val loss 4.5436\n",
            "Epoch 9 | Step 62800 | train_loss 4.5330 | val loss 4.5433\n",
            "Epoch 9 | Step 63000 | train_loss 4.5254 | val loss 4.5435\n",
            "Epoch 9 | Step 63200 | train_loss 4.5338 | val loss 4.5419\n",
            "Epoch 9 | Step 63400 | train_loss 4.5304 | val loss 4.5418\n",
            "Epoch 9 | Step 63600 | train_loss 4.5237 | val loss 4.5406\n",
            "Epoch 9 | Step 63800 | train_loss 4.5390 | val loss 4.5390\n",
            "Epoch 9 | Step 64000 | train_loss 4.5424 | val loss 4.5368\n",
            "Epoch 9 | Step 64200 | train_loss 4.5404 | val loss 4.5363\n",
            "Epoch 9 | Step 64400 | train_loss 4.5447 | val loss 4.5360\n",
            "Epoch 9 | Step 64600 | train_loss 4.5391 | val loss 4.5345\n",
            "Epoch 9 | Step 64800 | train_loss 4.5380 | val loss 4.5334\n",
            "Epoch 9 | Step 65000 | train_loss 4.5400 | val loss 4.5331\n",
            "Epoch 9 | Step 65200 | train_loss 4.5332 | val loss 4.5331\n",
            "Epoch 9 | Step 65400 | train_loss 4.5387 | val loss 4.5301\n",
            "Epoch 9 | Step 65600 | train_loss 4.5381 | val loss 4.5290\n",
            "Epoch 9 finished in 240.5s | val loss = 4.5277\n",
            "Saved best model.\n",
            "Epoch 10 | Step 65800 | train_loss 1.0306 | val loss 4.5323\n",
            "Epoch 10 | Step 66000 | train_loss 4.4717 | val loss 4.5319\n",
            "Epoch 10 | Step 66200 | train_loss 4.4915 | val loss 4.5332\n",
            "Epoch 10 | Step 66400 | train_loss 4.4832 | val loss 4.5323\n",
            "Epoch 10 | Step 66600 | train_loss 4.4841 | val loss 4.5328\n",
            "Epoch 10 | Step 66800 | train_loss 4.4881 | val loss 4.5312\n",
            "Epoch 10 | Step 67000 | train_loss 4.4890 | val loss 4.5309\n",
            "Epoch 10 | Step 67200 | train_loss 4.4931 | val loss 4.5313\n",
            "Epoch 10 | Step 67400 | train_loss 4.4957 | val loss 4.5299\n",
            "Epoch 10 | Step 67600 | train_loss 4.4973 | val loss 4.5290\n",
            "Epoch 10 | Step 67800 | train_loss 4.4816 | val loss 4.5293\n",
            "Epoch 10 | Step 68000 | train_loss 4.4880 | val loss 4.5298\n",
            "Epoch 10 | Step 68200 | train_loss 4.5087 | val loss 4.5261\n",
            "Epoch 10 | Step 68400 | train_loss 4.4874 | val loss 4.5287\n",
            "Epoch 10 | Step 68600 | train_loss 4.4953 | val loss 4.5273\n",
            "Epoch 10 | Step 68800 | train_loss 4.4996 | val loss 4.5254\n",
            "Epoch 10 | Step 69000 | train_loss 4.4878 | val loss 4.5270\n",
            "Epoch 10 | Step 69200 | train_loss 4.5032 | val loss 4.5233\n",
            "Epoch 10 | Step 69400 | train_loss 4.5030 | val loss 4.5225\n",
            "Epoch 10 | Step 69600 | train_loss 4.4964 | val loss 4.5221\n",
            "Epoch 10 | Step 69800 | train_loss 4.5102 | val loss 4.5219\n",
            "Epoch 10 | Step 70000 | train_loss 4.5070 | val loss 4.5193\n",
            "Epoch 10 | Step 70200 | train_loss 4.4974 | val loss 4.5174\n",
            "Epoch 10 | Step 70400 | train_loss 4.5025 | val loss 4.5163\n",
            "Epoch 10 | Step 70600 | train_loss 4.5000 | val loss 4.5156\n",
            "Epoch 10 | Step 70800 | train_loss 4.4992 | val loss 4.5155\n",
            "Epoch 10 | Step 71000 | train_loss 4.5077 | val loss 4.5150\n",
            "Epoch 10 | Step 71200 | train_loss 4.4995 | val loss 4.5136\n",
            "Epoch 10 | Step 71400 | train_loss 4.5058 | val loss 4.5122\n",
            "Epoch 10 | Step 71600 | train_loss 4.5059 | val loss 4.5124\n",
            "Epoch 10 | Step 71800 | train_loss 4.4997 | val loss 4.5124\n",
            "Epoch 10 | Step 72000 | train_loss 4.5100 | val loss 4.5111\n",
            "Epoch 10 | Step 72200 | train_loss 4.5056 | val loss 4.5098\n",
            "Epoch 10 | Step 72400 | train_loss 4.5041 | val loss 4.5071\n",
            "Epoch 10 | Step 72600 | train_loss 4.5154 | val loss 4.5089\n",
            "Epoch 10 | Step 72800 | train_loss 4.4942 | val loss 4.5059\n",
            "Epoch 10 | Step 73000 | train_loss 4.5115 | val loss 4.5057\n",
            "Epoch 10 finished in 241.1s | val loss = 4.5061\n",
            "Saved best model.\n"
          ]
        }
      ],
      "source": [
        "def estimate_loss():\n",
        "  model.eval()\n",
        "  losses = []\n",
        "  with torch.no_grad():\n",
        "    for x, y in val_loader:\n",
        "      x = x.to(device); y = y.to(device)\n",
        "      logits, _ = model(x)\n",
        "      loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
        "      losses.append(loss.item())\n",
        "    model.train()\n",
        "    return float(np.mean(losses))\n",
        "\n",
        "best_val = 1e9\n",
        "global_step = 0\n",
        "for epoch in range(1, epochs + 1):\n",
        "  t0 = time.time()\n",
        "  running_loss = 0.0\n",
        "  for it, (x,y) in enumerate(train_loader, 1):\n",
        "    x = x.to(device); y = y.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    logits, _ = model(x)\n",
        "    loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "    global_step += 1\n",
        "\n",
        "    if global_step % print_every == 0:\n",
        "      val_loss = estimate_loss()\n",
        "      print(f'Epoch {epoch} | Step {global_step} | train_loss {running_loss/print_every:.4f} | val loss {val_loss:.4f}')\n",
        "      running_loss = 0.0\n",
        "\n",
        "  val_loss = estimate_loss()\n",
        "  print(f'Epoch {epoch} finished in {time.time()-t0:.1f}s | val loss = {val_loss:.4f}')\n",
        "  if val_loss < best_val:\n",
        "    best_val = val_loss\n",
        "    torch.save(model.state_dict(), 'tiny_gpt.pt')\n",
        "    print(\"Saved best model.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXVZxB6YoXQu",
        "outputId": "ca55f386-ce39-4054-9220-ea8b1f28cb2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: David was a farmer who loved his family more than anything\n",
            "\n",
            "Generated Text:\n",
            "\n",
            "david was a farmer who loved his family more than anything and not wanting to do anything to pay it.\n",
            "\n",
            "in the , jim is the one who won ' t win the first time , he was going back.\n",
            "\n",
            "when the local gangster ' s car arrives , george ' s partner ' s body was left.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "user = \"David was a farmer who loved his family more than anything\"\n",
        "\n",
        "prompt_token = tokenizer(user)\n",
        "prompt_id = torch.tensor([[stoi.get(t, unk_idx) for t in prompt_token]], dtype=torch.long).to(device)\n",
        "\n",
        "print('Prompt:', user)\n",
        "\n",
        "out_idx = model.generate(prompt_id, max_new_tokens=50, temperature=1.0, top_k=50)\n",
        "\n",
        "# Decode token IDs to text\n",
        "generated_text = decode(out_idx[0].cpu().tolist())\n",
        "\n",
        "# ----- CLEANING -----\n",
        "generated_text = generated_text.replace(\"<unk>\", \"\")\n",
        "generated_text = generated_text.replace(\"<sep>\", \"\")\n",
        "generated_text = \" \".join(generated_text.split())   # remove double spaces\n",
        "\n",
        "# ----- PRINT SENTENCE BY SENTENCE -----\n",
        "sentences = generated_text.split('.')\n",
        "print(\"\\nGenerated Text:\\n\")\n",
        "for s in sentences:\n",
        "    s = s.strip()\n",
        "    if s:\n",
        "        print(s + \".\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
